
dupd is a duplicate detection utility.

% dupd operation options

    scan    scan starting from the given path
      --path PATH      path where scanning will start
      --nodb           do not generate database file
      --firstblocks N  max blocks to read in first hash pass
      --intblocks N    blocks to read in intermediate hash
      --skip-two       do not compare two files directly
      --skip-three     do not compare three files directly
      --file-count     max estimated number of files to scan
      --avg-size       estimated average file path length

    report  show duplicate report from last scan
      --cut PATHSEG    remove 'PATHSEG' from report paths
      --minsize SIZE   min size of duplicated space to report

    file    check for duplicates of the given file
      --file PATH      check this file

    help    show more help

General options include:
    -v      increase verbosity (may be repeated for even more)
    -q      quiet, supress all output except fatal errors
    --db    path to dupd database file


Scanning
--------
At a minimum the --path option must be specified. For example:

% dupd scan --path $HOME

This will scan all files in your home directory and in all contained
subdirectories. The duplicates found will be saved in a database file
$HOME/.dupd_sqlite.

The --path option can be repeated multiple times in order to scan
multiple directory hierarchies.

Additional options are available:

--db PATH          Override the default database file location.

-v                 Increases diagnostic verbosity. Can be repeated several
                   times for ever more output.

-q                 Quiet, suppress all output.

--nodb             Do not create the database file. Duplicate file info is
                   sent to stdout instead.

The remaining scan options can be used to experiment with alternate
scanning thresholds. In most cases these will not provide significant
performance difference.

dupd first scans all the files and groups them by size.  Files with a
unique size cannot have duplicates so these are discarded. After this
initial scan dupd has size groups containing two or more files.

By default, if there are only two files of a given size dupd will
compare them directly instead of computing their hashes because if the
files are different it will save having to completely read them.

--skip-two          This option disables the two file comparison. In general
                    there should be no reason to do so. The only exception is
                    if there are a large number of pairs of files which
                    are large and duplicates. In that case all of them will
                    need to be read completely so it can be more efficient
                    to read and hash one at a time.

By default dupd also compares three files of the same size directly, for
the same reasons.

--skip-three        This option disables the three file comparison.

Next, dupd will read only one 8K block of each file in a given size
group. This is effective at quickly discarding different files from
the list of potential duplicates.

If the files in a given size group are slightly above this size
threshold it may be faster to simply read them entirely during the
first pass instead of doing a separate pass on them later. By default
dupd will read up to 8 blocks of 8K each, or 64K, in this first pass
if the files size is smaller than this. For many random file
collections this has been seen to provide a reasonable tradeoff and
there is no significant benefit in changing this value:

--firstblocks N     Maximum number of blocks to read in the first pass.

Next, dupd can do an intermediate pass where it computes the hash on a
larger number of 8K blocks than the first round but not on the entire
file. Experimentation has shown that this is often not worth doing so
this intermediate pass is disabled by default. The case where this can
be benefitial is when the files being scanned are large with few
duplicates but they tend to be identical in the first 8K of
content. In such a scenario the intermediate scan can help.

--intblocks N      Blocks to read during the intermediate scan. Must be
                   larger than one to enable the intermediate scan.

Finally, for any files which are still undecided after all the above
steps, dupd will read and hash the entire file to compare against the
others in the size group. There is no configuration for this stage.


The two remaining options adjust the path block size which needs
enough capacity to hold all the full pathnames of all the files which
will be scanned. The path block is allocated once and not expanded.
The path block size will be (file-count * avg-size).

--file-count       Estimated maximum number of files to scan. The default
                   is a million files.

--avg-size         Estimated average file path length. Default is 512 bytes.



Reporting
---------
After a scan has completed, display the duplicates by running a report:

% dupd report

Options include:

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report
                   output. This can reduce clutter in the output text if
                   all the files scanned share a long identical prefix.

--minsize SIZE     Report only duplicate sets which consume at least this
                   much space. Note this is the total size occupied by
                   all the duplicates in a set, not their individual file size.

Check one file
--------------
To check whether one given file still has known duplicates use the
file operation.  Note that this does not do a new scan so it will not
find new duplicates. This checks whether the duplicates identified
during the previous scan still exist and are still duplicates. This is
convenient when the database contains a large scan which takes a long
time to re-run.

% dupd file --file PATH

Options include:

--file PATH        Required: The file to check

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report

