
dupd is a duplicate detection utility.

% dupd operation options

    scan    scan starting from the given path
      --path PATH         path where scanning will start
      --nodb              do not generate database file
      --firstblocks N     max blocks to read in first hash pass
      --firstblocksize N  size of firstblocks to read
      --intblocks N       blocks to read in intermediate hash
      --blocksize N       size of regular blocks to read
      --skip-two          do not compare two files directly
      --skip-three        do not compare three files directly
      --file-count        max estimated number of files to scan
      --avg-size          estimated average file path length
      --uniques           save info about unique files
      --stats-file FILE   save stats to this file
      --minsize SIZE      min size of files to scan

    report  show duplicate report from last scan
      --cut PATHSEG    remove 'PATHSEG' from report paths
      --minsize SIZE   min size of duplicated space to report

    file    based on report, check for duplicates of one file
      --file PATH      check this file
      --exclude PATH   if a duplicate lives under PATH, ignore

    uniques based on report, look for unique files
      --path PATH      path where scanning will start
      --exclude PATH   if a duplicate lives under PATH, ignore

    dups    based on report, look for duplicate files
      --path PATH      path where scanning will start
      --exclude PATH   if a duplicate lives under PATH, ignore

    ls      based on report, list info about every file seen
      --path PATH      path where scanning will start
      --exclude PATH   if a duplicate lives under PATH, ignore

    rmsh    create shell script to delete all duplicates
      --link           create symlinks for deleted files
      --hardlink       create hard links for deleted files

    help    show brief usage info

    usage   show more extensive documentation

    license show license info

    version show version and exit

General options include:
    -v          increase verbosity (may be repeated for more)
    -q          quiet, supress all output except fatal errors
    --db        path to dupd database file
    --no-unique ignore unique table even if present


Scanning
--------
At a minimum the --path option must be specified. For example:

% dupd scan --path $HOME

This will scan all files in your home directory and in all contained
subdirectories. The duplicates found will be saved in a database file
$HOME/.dupd_sqlite.

The --path option can be repeated multiple times in order to scan
multiple directory hierarchies.

Additional options are available:

--uniques          Save in a separate table the paths of all files found to
                   be unique while searching for duplicates. This will
                   increase the size of the database (possibly by a lot)
                   and slow down the scan. It can be useful if after the
                   scan you intend to take advantage of a list of unique
                   files in addition to the sets of duplicates.

--no-unique        Ignore the unique info in the database (if any).
                   Useful only for debugging.

--db PATH          Override the default database file location.

-v                 Increases diagnostic verbosity. Can be repeated several
                   times for ever more output.

-q                 Quiet, suppress all output.

--nodb             Do not create the database file. Duplicate file info is
                   sent to stdout instead.

--stats-file FILE  On completion, create (or append to) FILE and save some
                   stats from the run. These are the same stats as get
                   displayed with "-v -v" verbosity but are more friendly
                   for programmatic consumption.

--minsize SIZE     Ignore files smaller than this size when scanning.

The remaining scan options can be used to experiment with alternate
scanning thresholds. In most cases these will not provide significant
performance difference.

dupd first scans all the files and groups them by size.  Files with a
unique size cannot have duplicates so these are discarded. After this
initial scan dupd has size groups containing two or more files.

By default, if there are only two files of a given size dupd will
compare them directly instead of computing their hashes because if the
files are different it will save having to completely read them.

--skip-two          This option disables the two file comparison. In general
                    there should be no reason to do so. The only exception is
                    if there are a large number of pairs of files which
                    are large and duplicates. In that case all of them will
                    need to be read completely so it can be more efficient
                    to read and hash one at a time.

By default dupd also compares three files of the same size directly, for
the same reasons.

--skip-three        This option disables the three file comparison.

For size groups of four or more files, dupd will first read and hash a
small amount of data from each. This is effective at quickly
discarding different files from the list of potential duplicates.

By default, dupd will read a 512 byte block, or up to two blocks (1K)
if the file is smaller than that. These defaults can be changed with:

--firstblocksize N  Number of bytes in the initial block read.
--firstblocks N     Maximum number of blocks to read in the first pass.

Next, dupd can do an intermediate pass where it computes the hash on a
larger number of blocks than the first round but not on the entire
file.  Experimentation has shown that this is often not worth doing so
this intermediate pass is disabled by default. The case where this can
be benefitial is when the files being scanned are large with few
duplicates but they tend to be identical in the first bytes of
content. In such a scenario the intermediate scan may help.

--intblocks N      Blocks to read during the intermediate scan. Must be
                   larger than one to enable the intermediate scan.
--blocksize N      Block size in bytes. Default 8192.

Finally, for any files which are still undecided after all the above
steps, dupd will read and hash the entire file to compare against the
others in the size group. The same blocksize as above is used.


The two remaining options adjust the path block size which needs
enough capacity to hold all the full pathnames of all the files which
will be scanned. The path block is allocated once and not expanded.
The path block size will be (file-count * avg-size).

--file-count       Estimated maximum number of files to scan. The default
                   is a million files.

--avg-size         Estimated average file path length. Default is 512 bytes.



Reporting
---------
After a scan has completed, display the duplicates by running a report:

% dupd report

Options include:

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report
                   output. This can reduce clutter in the output text if
                   all the files scanned share a long identical prefix.

--minsize SIZE     Report only duplicate sets which consume at least this
                   much space. Note this is the total size occupied by
                   all the duplicates in a set, not their individual file size.

Note: The database format generated by scan is not guaranteed to be compatible
with future versions. You should run report (and all the other commands below
which access the database) using the same version of dupd that was used to
generate the database.


Check one file
--------------
To check whether one given file still has known duplicates use the
file operation.  Note that this does not do a new scan so it will not
find new duplicates. This checks whether the duplicates identified
during the previous scan still exist and verifies (by hash) whether
they are still duplicates. Note that if you only want to know whether the
file had duplicates at the time of the scan, it is faster to save the
report output to a file and search that.

% dupd file --file PATH

Options include:

--file PATH        Required: The file to check

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report

--exclude PATH     Ignore any duplicates under PATH when reporting duplicates.
                   This is useful if you intend to delete the entire tree
                   under PATH, to make sure you don't delete all copies
                   of the file.


List matching files
-------------------
These operations list the best-known status of each file on disk.
See 'file' operation for more info.

ls: List all files, show whether they have duplicates or not.
uniques: List all unique files.
dups: List all files which have known duplicates.

% dupd ls
% dupd uniques
% dupd dups

Options include:

--path PATH        Start from this directory (default .)

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report

--exclude PATH     Ignore any duplicates under PATH when reporting duplicates.


Remove duplicate files
----------------------
As a policy dupd never modifies the filesystem.

As a convenience for those times when it is desirable to automatically
remove files, this operation can create a shell script to do so. The
output is a shell script instead of directly deleting the files as an
additional layer of safety: Review the generated script carefully to
see if it truly does what you want!

Automated deletion is generally not very useful because it takes human
intervention to decide which of the duplicates is the best one to keep
in each case. While the content is the same, one of them may have a
better file name and/or location.

Optionally, the shell script can create either soft or hard links from
each removed file to the copy being kept. The options are mutually
exclusive (if both given, the last one takes precedence).

--link           create symlinks for deleted files
--hardlink       create hard links for deleted files

% dupd rmsh > delete_my_files.sh

