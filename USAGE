
dupd is a duplicate detection utility.

Scanning
--------
% dupd scan --path $HOME

This will scan all files in your home directory and in all contained
subdirectories. The duplicates found will be saved in a database file
$HOME/.dupd_sqlite.

The --path option can be repeated multiple times in order to scan
multiple directory hierarchies. If no --path option is given it defaults
to the current directory.

Additional options are available:

--hidden           Include hidden files and directories in the scan.
                   By default these are not included.

--uniques          Save in a separate table the paths of all files found to
                   be unique while searching for duplicates. This will
                   increase the size of the database (possibly by a lot)
                   and slow down the scan. It can be useful if after the
                   scan you intend to take advantage of a list of unique
                   files in addition to the sets of duplicates.

--no-unique        Ignore the unique info in the database (if any).
                   Useful only for debugging.

--db PATH          Override the default database file location.
                   If you override the path during scan, remember to
                   provide this argument and the path for subsequent
                   operations so the database can be found.

-v                 Increases diagnostic verbosity. Can be repeated several
                   times for ever more output.

-q                 Quiet, suppress all output.

--nodb             Do not create the database file. Duplicate file info is
                   sent to stdout instead.

--stats-file FILE  On completion, create (or append to) FILE and save some
                   stats from the run. These are the same stats as get
                   displayed with "-v -v" verbosity but are more friendly
                   for programmatic consumption.

--minsize SIZE     Ignore files smaller than this size when scanning.

--pathsep CHAR     Change the internal path separator character to CHAR.
                   When a list of paths is saved to the database, they are
                   separated by this character. The result is that any files
                   which contains this character in the name will be ignored
                   during scan because the name conflicts with this separator.

                   This is a bug, really, but unless you have oddly named
                   files should not be much of an issue. If you do have files
                   names containing the default separator (`), this option
                   allows changing it to something else that might work better
                   for your file set.

--no-thread-scan   Do filesystem scan phase in a single thread. The default
                   is to have two threads, one which traverses the directory
                   tree and the other one which builds the internal data
                   structures. The default is generally always faster.

--no-thread-hash   Do the hash and compare phase in a single thread. The
                   default is to have two threads, one which reads data from
                   disk and the other one which hashes and compares.

The remaining scan options can be used to experiment with alternate
scanning thresholds. In most cases these will not provide significant
performance difference.

dupd first scans all the files and groups them by size.  Files with a
unique size cannot have duplicates so these are discarded. After this
initial scan dupd has size groups containing two or more files.

By default, if there are only two files of a given size dupd will
compare them directly instead of computing their hashes because if the
files are different it will save having to completely read them.

--skip-two          This option disables the two file comparison. In general
                    there should be no reason to do so. The only exception is
                    if there are a large number of pairs of files which
                    are large and duplicates. In that case all of them will
                    need to be read completely so it can be more efficient
                    to read and hash one at a time.

By default dupd also compares three files of the same size directly, for
the same reasons.

--skip-three        This option disables the three file comparison.

When doing these direct file comparisons, the default block size
can be adjusted.

--fileblocksize N   Size of blocks to read in file compare.

For size groups of four or more files, dupd will first read and hash a
small amount of data from each. This is effective at quickly
discarding different files from the list of potential duplicates.

By default, dupd will first read a --firstblocksize byte block, or up
to --firstblocks blocks if the file is smaller than that. The defaults
can vary based on other options. Override defaults with:

--firstblocksize N  Number of bytes in the initial block read.
--firstblocks N     Maximum number of blocks to read in the first pass.

Next, dupd can do an intermediate pass where it computes the hash on a
larger number of blocks than the first round but not on the entire
file.  Experimentation has shown that this is often not worth doing so
this intermediate pass is disabled by default. The case where this can
be benefitial is when the files being scanned are large with few
duplicates but they tend to be identical in the first bytes of
content. In such a scenario the intermediate scan may help.

--intblocks N      Blocks to read during the intermediate scan. Must be
                   larger than one to enable the intermediate scan.
--blocksize N      Block size in bytes. Default 8192.

Finally, for any files which are still undecided after all the above
steps, dupd will read and hash the entire file to compare against the
others in the size group. The same blocksize as above is used.


The two remaining options adjust the path block size which needs
enough capacity to hold all the full pathnames of all the files which
will be scanned. The path block is allocated once and not expanded.
The path block size will be (file-count * avg-size).

--file-count       Estimated maximum number of files to scan. The default
                   is a million files.

--avg-size         Estimated average file path length. Default is 512 bytes.



Reporting
---------
After a scan has completed, display the duplicates by running a report:

% dupd report

Options include:

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report
                   output. This can reduce clutter in the output text if
                   all the files scanned share a long identical prefix.

--minsize SIZE     Report only duplicate sets which consume at least this
                   much space. Note this is the total size occupied by
                   all the duplicates in a set, not their individual file size.

Note: The database format generated by scan is not guaranteed to be compatible
with future versions. You should run report (and all the other commands below
which access the database) using the same version of dupd that was used to
generate the database.


Check one file
--------------
To check whether one given file still has known duplicates use the
file operation.  Note that this does not do a new scan so it will not
find new duplicates. This checks whether the duplicates identified
during the previous scan still exist and verifies (by hash) whether
they are still duplicates. Note that if you only want to know whether the
file had duplicates at the time of the scan, it is faster to save the
report output to a file and search that.

% dupd file --file PATH

Options include:

--file PATH        Required: The file to check

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report

--exclude PATH     Ignore any duplicates under PATH when reporting duplicates.
                   This is useful if you intend to delete the entire tree
                   under PATH, to make sure you don't delete all copies
                   of the file.


List matching files
-------------------
These operations list the best-known status of each file on disk.
See 'file' operation for more info.

ls: List all files, show whether they have duplicates or not.
uniques: List all unique files.
dups: List all files which have known duplicates.

% dupd ls
% dupd uniques
% dupd dups

Options include:

--path PATH        Start from this directory (default .)

--db PATH          Override the default database file location.

--cut PATHSEG      Remove prefix $PATHSEG from the file paths in the report

--exclude PATH     Ignore any duplicates under PATH when reporting duplicates.


Refreshing the database
-----------------------
As you remove duplicate files, the list of duplicates in the database grows
increasingly stale. Whenever possible you should re-run a scan operation
to refresh the database content after having done some cleanup.

Unfortunately this might not be very convenient when working with a
large file set where the scan operation takes a long time.

The refresh command offers an alternative, with caveats. It checks all
the duplicate files listed in the database and removes those which are
no longer present in the filesystem.

Be sure to consider the limitations of this approach. The refresh command
DOES NOT re-verify whether all files listed as duplicates are still
duplicates.  It also, of course, does not detect any new duplicates which
may have appeared since the last scan.

% dupd refresh


Remove duplicate files
----------------------
As a policy dupd never modifies the filesystem.

As a convenience for those times when it is desirable to automatically
remove files, this operation can create a shell script to do so. The
output is a shell script instead of directly deleting the files as an
additional layer of safety: Review the generated script carefully to
see if it truly does what you want!

Automated deletion is generally not very useful because it takes human
intervention to decide which of the duplicates is the best one to keep
in each case. While the content is the same, one of them may have a
better file name and/or location.

Optionally, the shell script can create either soft or hard links from
each removed file to the copy being kept. The options are mutually
exclusive (if both given, --hardlink takes precedence).

--link           create symlinks for deleted files
--hardlink       create hard links for deleted files

% dupd rmsh > delete_my_files.sh

